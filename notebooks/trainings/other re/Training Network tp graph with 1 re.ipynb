{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "313113c0",
   "metadata": {},
   "source": [
    "import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e457128",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "import dgl\n",
    "from sklearn.metrics import f1_score\n",
    "import torch as th\n",
    "import dgl.nn.pytorch as dglnn\n",
    "import torch.nn as nn\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "import warnings\n",
    "from feature_extraction.base_featurizer import BaseFeaturizer\n",
    "import torch\n",
    "from graph_construction.tps_graph import create_dummy_dgl_graph, tps_graph\n",
    "import numpy as np\n",
    "\n",
    "from dgl_classifier.trainer import GraphDataset,snap_pred, tps_to_dgl, get_clasification_vec,\\\n",
    "tps_graph_const, extract_data, runner\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "from preprocessing.utils import load_BGPS_from_json\n",
    "warnings.filterwarnings('ignore', category=UserWarning, message='TypedStorage is deprecated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88ee7fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed\n",
    "dgl.seed(1223)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0439ecf4",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52f52381",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = dglnn.RelGraphConv(in_dim, hidden_dim, 8)\n",
    "        #self.conv1 = dglnn.GraphConv(in_dim, hidden_Ë‡dim)\n",
    "        self.conv2 = dglnn.RelGraphConv(hidden_dim, hidden_dim, 8)\n",
    "        #self.conv2 = dglnn.GraphConv(hidden_dim, hidden_dim)\n",
    "        self.classify = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, g, h, rel_types):\n",
    "        # Apply graph convolution and activation.\n",
    "        h = F.relu(self.conv1(g, h, rel_types))\n",
    "        h = F.relu(self.conv2(g, h, rel_types))\n",
    "        with g.local_scope():\n",
    "            g.ndata['node_features'] = h\n",
    "            # Calculate graph representation by average readout.\n",
    "            hg = dgl.mean_nodes(g, 'node_features')\n",
    "            return F.softmax( self.classify(hg), dim=1)\n",
    "model = Classifier(113, 20, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10098d4",
   "metadata": {},
   "source": [
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5812089",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop =10\n",
    "model = Classifier(113, 20, 2)\n",
    "lr = 0.01\n",
    "wd = 5e-4\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59221c9a",
   "metadata": {},
   "source": [
    "file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90076eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '/work/data/splits/train.json'\n",
    "val_file = '/work/data/splits/val.json'\n",
    "test_file = '/work/data/splits/test.json'\n",
    "path_to_save = '/work/data/models'\n",
    "path_to_res = '/work/data/results/25_5_2023_tp_graph_1re.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9ae22ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/dgl/backend/pytorch/tensor.py:52: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  return th.as_tensor(data, dtype=dtype)\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, val_dataloader, test_dataloader = extract_data(train_file, val_file, test_file, community_no=30, batch_size = 50, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d542c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------------------------------------------------\n",
      "Epoch:    1        1 Batch loss: 0.704280 Batch F1: 0.11764705882352941\n",
      "Epoch:    1        2 Batch loss: 0.683773 Batch F1: 0.5306122448979592\n",
      "Epoch:    1        3 Batch loss: 0.664188 Batch F1: 0.6349206349206348\n",
      "Epoch:    1        4 Batch loss: 0.650744 Batch F1: 0.7666666666666667\n",
      "Epoch:    1        5 Batch loss: 0.632558 Batch F1: 0.7\n",
      "Epoch:    1        6 Batch loss: 0.556272 Batch F1: 0.8571428571428571\n",
      "Train Avg Loss    1: 0.648636\n",
      "\n",
      "Train Avg F1    1: 0.601164910408608\n",
      "\n",
      "Val Avg Loss    1: 0.207119\n",
      "\n",
      "Val Avg F1    1:  0.6329411764705883\n",
      "\n",
      "Optimal Val loss (Epoch 1): 0.20711884647607803\n",
      "\n",
      "Epoch 2\n",
      "--------------------------------------------------------------\n",
      "Epoch:    2        1 Batch loss: 0.637679 Batch F1: 0.5957446808510638\n",
      "Epoch:    2        2 Batch loss: 0.628127 Batch F1: 0.5333333333333333\n",
      "Epoch:    2        3 Batch loss: 0.536864 Batch F1: 0.7200000000000001\n",
      "Epoch:    2        4 Batch loss: 0.592621 Batch F1: 0.7575757575757576\n",
      "Epoch:    2        5 Batch loss: 0.604732 Batch F1: 0.7272727272727274\n",
      "Epoch:    2        6 Batch loss: 0.665232 Batch F1: 0.5\n",
      "Train Avg Loss    2: 0.610876\n",
      "\n",
      "Train Avg F1    2: 0.6389877498388138\n",
      "\n",
      "Val Avg Loss    2: 0.224159\n",
      "\n",
      "Val Avg F1    2:  0.7171428571428571\n",
      "\n",
      "Optimal Val loss (Epoch 1): 0.20711884647607803\n",
      "\n",
      "Epoch 3\n",
      "--------------------------------------------------------------\n",
      "Epoch:    3        1 Batch loss: 0.515640 Batch F1: 0.847457627118644\n",
      "Epoch:    3        2 Batch loss: 0.572239 Batch F1: 0.7111111111111111\n",
      "Epoch:    3        3 Batch loss: 0.595372 Batch F1: 0.6341463414634146\n",
      "Epoch:    3        4 Batch loss: 0.638853 Batch F1: 0.5581395348837209\n",
      "Epoch:    3        5 Batch loss: 0.574699 Batch F1: 0.608695652173913\n",
      "Epoch:    3        6 Batch loss: 0.665333 Batch F1: 0.4\n",
      "Train Avg Loss    3: 0.593690\n",
      "\n",
      "Train Avg F1    3: 0.626591711125134\n",
      "\n",
      "Val Avg Loss    3: 0.219192\n",
      "\n",
      "Val Avg F1    3:  0.7519623233908947\n",
      "\n",
      "Optimal Val loss (Epoch 1): 0.20711884647607803\n",
      "\n",
      "Epoch 4\n",
      "--------------------------------------------------------------\n",
      "Epoch:    4        1 Batch loss: 0.481349 Batch F1: 0.8372093023255814\n",
      "Epoch:    4        2 Batch loss: 0.596432 Batch F1: 0.7307692307692308\n",
      "Epoch:    4        3 Batch loss: 0.619053 Batch F1: 0.5789473684210527\n",
      "Epoch:    4        4 Batch loss: 0.519593 Batch F1: 0.835820895522388\n",
      "Epoch:    4        5 Batch loss: 0.547694 Batch F1: 0.7692307692307692\n",
      "Epoch:    4        6 Batch loss: 0.337987 Batch F1: 1.0\n",
      "Train Avg Loss    4: 0.517018\n",
      "\n",
      "Train Avg F1    4: 0.791996261044837\n",
      "\n",
      "Val Avg Loss    4: 0.226965\n",
      "\n",
      "Val Avg F1    4:  0.7387914230019493\n",
      "\n",
      "Optimal Val loss (Epoch 1): 0.20711884647607803\n",
      "\n",
      "Epoch 5\n",
      "--------------------------------------------------------------\n",
      "Epoch:    5        1 Batch loss: 0.508369 Batch F1: 0.870967741935484\n",
      "Epoch:    5        2 Batch loss: 0.524518 Batch F1: 0.8\n",
      "Epoch:    5        3 Batch loss: 0.531840 Batch F1: 0.7450980392156864\n",
      "Epoch:    5        4 Batch loss: 0.609313 Batch F1: 0.6808510638297872\n",
      "Epoch:    5        5 Batch loss: 0.567879 Batch F1: 0.7200000000000001\n",
      "Epoch:    5        6 Batch loss: 0.373255 Batch F1: 1.0\n",
      "Train Avg Loss    5: 0.519196\n",
      "\n",
      "Train Avg F1    5: 0.802819474163493\n",
      "\n",
      "Val Avg Loss    5: 0.214993\n",
      "\n",
      "Val Avg F1    5:  0.7457264957264957\n",
      "\n",
      "Optimal Val loss (Epoch 1): 0.20711884647607803\n",
      "\n",
      "Epoch 6\n",
      "--------------------------------------------------------------\n",
      "Epoch:    6        1 Batch loss: 0.505684 Batch F1: 0.7916666666666666\n",
      "Epoch:    6        2 Batch loss: 0.478469 Batch F1: 0.8627450980392156\n",
      "Epoch:    6        3 Batch loss: 0.497497 Batch F1: 0.8333333333333333\n",
      "Epoch:    6        4 Batch loss: 0.529611 Batch F1: 0.7755102040816326\n",
      "Epoch:    6        5 Batch loss: 0.560072 Batch F1: 0.7636363636363638\n",
      "Epoch:    6        6 Batch loss: 0.705149 Batch F1: 0.7272727272727272\n",
      "Train Avg Loss    6: 0.546081\n",
      "\n",
      "Train Avg F1    6: 0.7923607321716566\n",
      "\n",
      "Val Avg Loss    6: 0.208187\n",
      "\n",
      "Val Avg F1    6:  0.7696078431372548\n",
      "\n",
      "Optimal Val loss (Epoch 1): 0.20711884647607803\n",
      "\n",
      "Epoch 7\n",
      "--------------------------------------------------------------\n",
      "Epoch:    7        1 Batch loss: 0.525017 Batch F1: 0.7999999999999999\n",
      "Epoch:    7        2 Batch loss: 0.579855 Batch F1: 0.72\n",
      "Epoch:    7        3 Batch loss: 0.469796 Batch F1: 0.8666666666666666\n",
      "Epoch:    7        4 Batch loss: 0.479368 Batch F1: 0.8461538461538461\n",
      "Epoch:    7        5 Batch loss: 0.526780 Batch F1: 0.7727272727272727\n",
      "Epoch:    7        6 Batch loss: 0.452901 Batch F1: 0.9090909090909091\n",
      "Train Avg Loss    7: 0.505619\n",
      "\n",
      "Train Avg F1    7: 0.8191064491064491\n",
      "\n",
      "Val Avg Loss    7: 0.209579\n",
      "\n",
      "Val Avg F1    7:  0.7515813686026452\n",
      "\n",
      "Optimal Val loss (Epoch 1): 0.20711884647607803\n",
      "\n",
      "Epoch 8\n",
      "--------------------------------------------------------------\n",
      "Epoch:    8        1 Batch loss: 0.531496 Batch F1: 0.7659574468085106\n",
      "Epoch:    8        2 Batch loss: 0.526020 Batch F1: 0.711111111111111\n",
      "Epoch:    8        3 Batch loss: 0.607645 Batch F1: 0.6956521739130435\n",
      "Epoch:    8        4 Batch loss: 0.492401 Batch F1: 0.823529411764706\n",
      "Epoch:    8        5 Batch loss: 0.437325 Batch F1: 0.9019607843137256\n",
      "Epoch:    8        6 Batch loss: 0.382606 Batch F1: 0.8571428571428571\n",
      "Train Avg Loss    8: 0.496249\n",
      "\n",
      "Train Avg F1    8: 0.792558964175659\n",
      "\n",
      "Val Avg Loss    8: 0.220126\n",
      "\n",
      "Val Avg F1    8:  0.7842377260981912\n",
      "\n",
      "Optimal Val loss (Epoch 1): 0.20711884647607803\n",
      "\n",
      "Epoch 9\n",
      "--------------------------------------------------------------\n",
      "Epoch:    9        1 Batch loss: 0.461455 Batch F1: 0.9\n",
      "Epoch:    9        2 Batch loss: 0.655502 Batch F1: 0.7017543859649122\n",
      "Epoch:    9        3 Batch loss: 0.575782 Batch F1: 0.7346938775510204\n",
      "Epoch:    9        4 Batch loss: 0.518633 Batch F1: 0.8\n",
      "Epoch:    9        5 Batch loss: 0.490909 Batch F1: 0.8524590163934426\n",
      "Epoch:    9        6 Batch loss: 0.319270 Batch F1: 1.0\n",
      "Train Avg Loss    9: 0.503592\n",
      "\n",
      "Train Avg F1    9: 0.8314845466515625\n",
      "\n",
      "Val Avg Loss    9: 0.228194\n",
      "\n",
      "Val Avg F1    9:  0.7266457680250783\n",
      "\n",
      "Optimal Val loss (Epoch 1): 0.20711884647607803\n",
      "\n",
      "Epoch 10\n",
      "--------------------------------------------------------------\n",
      "Epoch:   10        1 Batch loss: 0.615044 Batch F1: 0.6666666666666666\n",
      "Epoch:   10        2 Batch loss: 0.458114 Batch F1: 0.8108108108108109\n",
      "Epoch:   10        3 Batch loss: 0.494447 Batch F1: 0.830188679245283\n",
      "Epoch:   10        4 Batch loss: 0.515074 Batch F1: 0.8148148148148148\n",
      "Epoch:   10        5 Batch loss: 0.469219 Batch F1: 0.8571428571428571\n",
      "Epoch:   10        6 Batch loss: 0.696717 Batch F1: 0.6666666666666665\n",
      "Train Avg Loss   10: 0.541436\n",
      "\n",
      "Train Avg F1   10: 0.7743817492245165\n",
      "\n",
      "Val Avg Loss   10: 0.225343\n",
      "\n",
      "Val Avg F1   10:  0.7515151515151515\n",
      "\n",
      "Optimal Val loss (Epoch 1): 0.20711884647607803\n",
      "\n",
      "Epoch 11\n",
      "--------------------------------------------------------------\n",
      "Epoch:   11        1 Batch loss: 0.573838 Batch F1: 0.7547169811320755\n",
      "Epoch:   11        2 Batch loss: 0.488264 Batch F1: 0.830188679245283\n",
      "Epoch:   11        3 Batch loss: 0.493293 Batch F1: 0.8\n",
      "Epoch:   11        4 Batch loss: 0.415239 Batch F1: 0.9019607843137255\n",
      "Epoch:   11        5 Batch loss: 0.475566 Batch F1: 0.851063829787234\n",
      "Epoch:   11        6 Batch loss: 0.585383 Batch F1: 0.8000000000000002\n",
      "Train Avg Loss   11: 0.505264\n",
      "\n",
      "Train Avg F1   11: 0.8229883790797197\n",
      "\n",
      "Val Avg Loss   11: 0.216620\n",
      "\n",
      "Val Avg F1   11:  0.7788359788359788\n",
      "\n",
      "Optimal Val loss (Epoch 1): 0.20711884647607803\n",
      "\n",
      "Epoch 12\n",
      "--------------------------------------------------------------\n",
      "Epoch:   12        1 Batch loss: 0.471597 Batch F1: 0.8627450980392156\n",
      "Epoch:   12        2 Batch loss: 0.541897 Batch F1: 0.7755102040816326\n",
      "Epoch:   12        3 Batch loss: 0.488392 Batch F1: 0.8\n",
      "Epoch:   12        4 Batch loss: 0.456077 Batch F1: 0.8727272727272727\n",
      "Epoch:   12        5 Batch loss: 0.472931 Batch F1: 0.8461538461538461\n",
      "Epoch:   12        6 Batch loss: 0.438407 Batch F1: 0.923076923076923\n",
      "Train Avg Loss   12: 0.478217\n",
      "\n",
      "Train Avg F1   12: 0.8467022240131484\n",
      "\n",
      "Val Avg Loss   12: 0.208430\n",
      "\n",
      "Val Avg F1   12:  0.7518557794273595\n",
      "\n",
      "Optimal Val loss (Epoch 1): 0.20711884647607803\n",
      "\n",
      "Epoch 13\n",
      "--------------------------------------------------------------\n",
      "Epoch:   13        1 Batch loss: 0.453235 Batch F1: 0.85\n",
      "Epoch:   13        2 Batch loss: 0.522282 Batch F1: 0.7916666666666666\n",
      "Epoch:   13        3 Batch loss: 0.496531 Batch F1: 0.823529411764706\n",
      "Epoch:   13        4 Batch loss: 0.495042 Batch F1: 0.830188679245283\n",
      "Epoch:   13        5 Batch loss: 0.466230 Batch F1: 0.8620689655172414\n",
      "Epoch:   13        6 Batch loss: 0.317751 Batch F1: 1.0\n",
      "Train Avg Loss   13: 0.458512\n",
      "\n",
      "Train Avg F1   13: 0.8595756205323162\n",
      "\n",
      "Val Avg Loss   13: 0.203325\n",
      "\n",
      "Val Avg F1   13:  0.79002079002079\n",
      "\n",
      "Optimal Val loss (Epoch 13): 0.20332524180412292\n",
      "\n",
      "Epoch 14\n",
      "--------------------------------------------------------------\n",
      "Epoch:   14        1 Batch loss: 0.450606 Batch F1: 0.8771929824561403\n",
      "Epoch:   14        2 Batch loss: 0.550867 Batch F1: 0.6666666666666666\n",
      "Epoch:   14        3 Batch loss: 0.458818 Batch F1: 0.8771929824561403\n",
      "Epoch:   14        4 Batch loss: 0.479142 Batch F1: 0.8666666666666666\n",
      "Epoch:   14        5 Batch loss: 0.423950 Batch F1: 0.888888888888889\n",
      "Epoch:   14        6 Batch loss: 0.690486 Batch F1: 0.5714285714285715\n",
      "Train Avg Loss   14: 0.508978\n",
      "\n",
      "Train Avg F1   14: 0.7913394597605125\n",
      "\n",
      "Val Avg Loss   14: 0.202813\n",
      "\n",
      "Val Avg F1   14:  0.7868937048503613\n",
      "\n",
      "Optimal Val loss (Epoch 14): 0.20281264930963516\n",
      "\n",
      "Epoch 15\n",
      "--------------------------------------------------------------\n",
      "Epoch:   15        1 Batch loss: 0.456153 Batch F1: 0.851063829787234\n",
      "Epoch:   15        2 Batch loss: 0.484230 Batch F1: 0.8\n",
      "Epoch:   15        3 Batch loss: 0.530425 Batch F1: 0.819672131147541\n",
      "Epoch:   15        4 Batch loss: 0.424856 Batch F1: 0.9056603773584904\n",
      "Epoch:   15        5 Batch loss: 0.493587 Batch F1: 0.7804878048780488\n",
      "Epoch:   15        6 Batch loss: 0.442746 Batch F1: 0.923076923076923\n",
      "Train Avg Loss   15: 0.472000\n",
      "\n",
      "Train Avg F1   15: 0.8466601777080397\n",
      "\n",
      "Val Avg Loss   15: 0.212154\n",
      "\n",
      "Val Avg F1   15:  0.7598091198303287\n",
      "\n",
      "Optimal Val loss (Epoch 14): 0.20281264930963516\n",
      "\n",
      "Epoch 16\n",
      "--------------------------------------------------------------\n",
      "Epoch:   16        1 Batch loss: 0.469515 Batch F1: 0.8461538461538461\n",
      "Epoch:   16        2 Batch loss: 0.558207 Batch F1: 0.7857142857142856\n",
      "Epoch:   16        3 Batch loss: 0.470412 Batch F1: 0.8260869565217391\n",
      "Epoch:   16        4 Batch loss: 0.426229 Batch F1: 0.8979591836734695\n",
      "Epoch:   16        5 Batch loss: 0.440315 Batch F1: 0.8627450980392156\n",
      "Epoch:   16        6 Batch loss: 0.564767 Batch F1: 0.8\n",
      "Train Avg Loss   16: 0.488241\n",
      "\n",
      "Train Avg F1   16: 0.8364432283504261\n",
      "\n",
      "Val Avg Loss   16: 0.192304\n",
      "\n",
      "Val Avg F1   16:  0.8207941483803554\n",
      "\n",
      "Optimal Val loss (Epoch 16): 0.19230380654335022\n",
      "\n",
      "Epoch 17\n",
      "--------------------------------------------------------------\n",
      "Epoch:   17        1 Batch loss: 0.561392 Batch F1: 0.7796610169491525\n",
      "Epoch:   17        2 Batch loss: 0.431554 Batch F1: 0.8888888888888888\n",
      "Epoch:   17        3 Batch loss: 0.482703 Batch F1: 0.8461538461538461\n",
      "Epoch:   17        4 Batch loss: 0.434730 Batch F1: 0.875\n",
      "Epoch:   17        5 Batch loss: 0.452861 Batch F1: 0.8571428571428572\n",
      "Epoch:   17        6 Batch loss: 0.479468 Batch F1: 0.6666666666666666\n",
      "Train Avg Loss   17: 0.473785\n",
      "\n",
      "Train Avg F1   17: 0.8189188793002352\n",
      "\n",
      "Val Avg Loss   17: 0.208352\n",
      "\n",
      "Val Avg F1   17:  0.7678571428571429\n",
      "\n",
      "Optimal Val loss (Epoch 16): 0.19230380654335022\n",
      "\n",
      "Epoch 18\n",
      "--------------------------------------------------------------\n",
      "Epoch:   18        1 Batch loss: 0.517157 Batch F1: 0.8076923076923077\n",
      "Epoch:   18        2 Batch loss: 0.414385 Batch F1: 0.9090909090909091\n",
      "Epoch:   18        3 Batch loss: 0.462996 Batch F1: 0.8205128205128205\n",
      "Epoch:   18        4 Batch loss: 0.559867 Batch F1: 0.76\n",
      "Epoch:   18        5 Batch loss: 0.443464 Batch F1: 0.8928571428571429\n",
      "Epoch:   18        6 Batch loss: 0.570771 Batch F1: 0.7499999999999999\n",
      "Train Avg Loss   18: 0.494773\n",
      "\n",
      "Train Avg F1   18: 0.8233588633588633\n",
      "\n",
      "Val Avg Loss   18: 0.199261\n",
      "\n",
      "Val Avg F1   18:  0.7891231964483908\n",
      "\n",
      "Optimal Val loss (Epoch 16): 0.19230380654335022\n",
      "\n",
      "Epoch 19\n",
      "--------------------------------------------------------------\n",
      "Epoch:   19        1 Batch loss: 0.460510 Batch F1: 0.851063829787234\n",
      "Epoch:   19        2 Batch loss: 0.471514 Batch F1: 0.8333333333333333\n",
      "Epoch:   19        3 Batch loss: 0.486165 Batch F1: 0.830188679245283\n",
      "Epoch:   19        4 Batch loss: 0.484291 Batch F1: 0.851851851851852\n",
      "Epoch:   19        5 Batch loss: 0.467562 Batch F1: 0.8461538461538461\n",
      "Epoch:   19        6 Batch loss: 0.563240 Batch F1: 0.7499999999999999\n",
      "Train Avg Loss   19: 0.488880\n",
      "\n",
      "Train Avg F1   19: 0.8270985900619247\n",
      "\n",
      "Val Avg Loss   19: 0.199532\n",
      "\n",
      "Val Avg F1   19:  0.8119047619047619\n",
      "\n",
      "Optimal Val loss (Epoch 16): 0.19230380654335022\n",
      "\n",
      "Epoch 20\n",
      "--------------------------------------------------------------\n",
      "Epoch:   20        1 Batch loss: 0.444868 Batch F1: 0.8771929824561403\n",
      "Epoch:   20        2 Batch loss: 0.496619 Batch F1: 0.816326530612245\n",
      "Epoch:   20        3 Batch loss: 0.505047 Batch F1: 0.8\n",
      "Epoch:   20        4 Batch loss: 0.457246 Batch F1: 0.8771929824561403\n",
      "Epoch:   20        5 Batch loss: 0.456971 Batch F1: 0.8627450980392156\n",
      "Epoch:   20        6 Batch loss: 0.570149 Batch F1: 0.75\n",
      "Train Avg Loss   20: 0.488483\n",
      "\n",
      "Train Avg F1   20: 0.8305762655939568\n",
      "\n",
      "Val Avg Loss   20: 0.231813\n",
      "\n",
      "Val Avg F1   20:  0.7361111111111112\n",
      "\n",
      "Optimal Val loss (Epoch 16): 0.19230380654335022\n",
      "\n",
      "Early Stopping invoked after epoch 20\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "runner(train_dataloader,val_dataloader,test_dataloader,model, early_stop, lr, wd, epochs, path_to_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71514a10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b7c830",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
